{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74af7d26-85de-44f3-83ac-e98340ac2d0f",
   "metadata": {},
   "source": [
    "## First\n",
    "\n",
    "- Copy `NEAR_regex.py` into the same folder as this file. [It's here](https://ledatascifi.github.io/ledatascifi-2023/content/04/02d_RegexApplication.html#demo) (click the \"+\") or in the community codebook. You should name the file `NEAR_regex.py` and not `NEAR_regex.ipynb`.\n",
    "- Make a `.gitignore` file in this folder with  `**10k_files/*` in it.\n",
    "- Copy [this file](https://github.com/donbowen/Class-Notes-1045/raw/main/Midterm%20sandbox/10k_files.zip) into the `10k_files/` folder here.\n",
    "- Copy the things in the assignment's input folder in to the inputs folder here.\n",
    "- Optional: You can install `tqdm` (If you don't, then remove it from the code below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908e0c86-ca84-47f8-95e7-e34c024ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex  # copy this file into the asgn folder\n",
    "from tqdm import tqdm  # progress bar on loops\n",
    "\n",
    "# if you have tqdm issues, run this in terminal or with ! trick\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#\n",
    "# if that fails, you can disable it\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fec2c-a2bd-4513-9430-33fd2401cdec",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9c226f-b543-4c03-a3d6-0dc1c77c3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - load the ML negative words into a list called BHR_negative\n",
    "# BHR is the author names on that paper\n",
    "# \"ML\" might be a better name, but having \"LM\" and \"ML\" in bound\n",
    "# to cause transcription errors\n",
    "\n",
    "BHR_negative = pd.read_csv('inputs/ML_negative_unigram.txt',\n",
    "            names=['word'])['word'].to_list()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ada34b-0a53-4101-ba01-c7701948300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - load the ML positive words into a list called BHR_positive\n",
    "\n",
    "# \"r\" means we are opening the file in \"read\" mode ... \"w\"rite mode means you'll write the file \n",
    "with open('inputs/ML_positive_unigram.txt', 'r') as file:\n",
    "    BHR_positive = [line.strip() for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f17c149-e025-46e2-9782-5009c9154cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - load the LM negative words into a list called LM_positive\n",
    "\n",
    "LM = pd.read_csv('inputs/LM_MasterDictionary_1993-2021.csv')\n",
    "LM_negative = LM.query('Negative > 0')['Word'].to_list()\n",
    "\n",
    "# ABCD > look at the data... what is going on with the values?\n",
    "# LM.describe() > there are negative numbers? and 2009-2014?\n",
    "# look for documentation! readme in the inputs folder points to the docs and explains the numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac2f3f-35be-4142-8d0d-1706c214861e",
   "metadata": {},
   "source": [
    "Look at the resulting lists... not identical structure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34694990-4f96-441f-a90e-8a770b8c47d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABANDON',\n",
       " 'ABANDONED',\n",
       " 'ABANDONING',\n",
       " 'ABANDONMENT',\n",
       " 'ABANDONMENTS',\n",
       " 'ABANDONS',\n",
       " 'ABDICATED',\n",
       " 'ABDICATES',\n",
       " 'ABDICATING',\n",
       " 'ABDICATION',\n",
       " 'ABDICATIONS',\n",
       " 'ABERRANT',\n",
       " 'ABERRATION',\n",
       " 'ABERRATIONAL',\n",
       " 'ABERRATIONS',\n",
       " 'ABETTING',\n",
       " 'ABNORMAL',\n",
       " 'ABNORMALITIES',\n",
       " 'ABNORMALITY',\n",
       " 'ABNORMALLY']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BHR_positive[:20]\n",
    "LM_negative[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a3bfea-9769-4713-9243-d47f546881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - load the LM positive words into a list called LM_positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcdd66-e646-4a64-bf26-a30092d1d970",
   "metadata": {},
   "source": [
    "## Looping over a dataframe and adding a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8894af2c-986e-4ec5-b642-96ca502f5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, URL: blahblah.com\n",
      "Index: 1, URL: wikisomething.com\n",
      "Index: 2, URL: wiki.com\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Security</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M</td>\n",
       "      <td>blahblah.com</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLSA</td>\n",
       "      <td>wikisomething.com</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APPL</td>\n",
       "      <td>wiki.com</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Security                URL  Sentiment\n",
       "0       3M       blahblah.com        1.0\n",
       "1     TLSA  wikisomething.com        2.0\n",
       "2     APPL           wiki.com        5.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# step 1 will load some database and prep it for the loopy parts\n",
    "# here, we will just use a toy dataset\n",
    "\n",
    "toy_database = pd.DataFrame({\"Security\":['3M','TLSA','APPL'],\n",
    "             \"URL\":['blahblah.com','wikisomething.com','wiki.com']})\n",
    "\n",
    "toy_database\n",
    "\n",
    "for index, row in toy_database.iterrows():\n",
    "    print(f\"Index: {index}, URL: {row['URL']}\")\n",
    "    #row can be used like row['var_name']\n",
    "    \n",
    "    sentiment_positive= random.randint(0,10)\n",
    "    toy_database.loc[index, 'Sentiment'] = sentiment_positive \n",
    "\n",
    "toy_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f890f76-b14b-43af-bc30-945d8300d48e",
   "metadata": {},
   "source": [
    "## Measure sentiment\n",
    "\n",
    "What fraction of the words in this \"document\" (sentence) are \"happy\" words?\n",
    "\n",
    "Answer: 2/13. Let's replicate that with code. \n",
    "\n",
    "First, count the length of the document.\n",
    "\n",
    "Then, count how many times each word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a57e4fb0-1b06-4406-9127-545cbd3ef723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sentiment = ['happy','smile','hopeful']\n",
    "\n",
    "sentence = 'I am happy that you are here. I am all smiles. So hopeful!'\n",
    "\n",
    "# q0 count the number of \"words\" (the doc length)\n",
    "\n",
    "len(sentence.split())\n",
    "# len(re.findall(' ',sentence))+1 # works only if every word in doc is separated by exactly 1 space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64bd1b-d09e-46cc-a4c6-09151217a530",
   "metadata": {},
   "source": [
    "What we did:\n",
    "```python \n",
    "# just type the words\n",
    "len(re.findall('happy',sentence)) # works but\n",
    "len(re.findall('smile',sentence)) # fails\n",
    "\n",
    "# look for words between word boundaries (\\b)\n",
    "# r before string is important!\n",
    "len(re.findall(r'\\bhappy\\b',sentence)) # works\n",
    "len(re.findall(r'\\bsmile\\b',sentence)) # works too\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70054e4-4ef9-47d3-ae7d-aabff017a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q1 count how many times \"happy\" is used in the doc\n",
    "# hint: https://ledatascifi.github.io/ledatascifi-2023/content/04/02b_regex.html\n",
    "\n",
    "len(re.findall(r'\\bhappy\\b',sentence))\n",
    "\n",
    "# r means \"raw\" string -- a backslash is a backslash, not the start of some special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c4a25d-c2af-4d6f-aa45-d457492be2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q2 count how many times \"smile\" is used in the doc\n",
    "\n",
    "# fails! smile is not a word in the doc!\n",
    "len(re.findall(r'smile',sentence))\n",
    "len(re.findall(r'\\b(smile)\\b',sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ee35ab-cf0d-4c52-b294-70f5a8c1344a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q3 count how many times \"smile\" or \"happy\" is used in the doc\n",
    "# hint: similar to q2 answer... \n",
    "# the answer is somewhere this page: https://regexone.com/\n",
    "\n",
    "# () makes for groups of matching possibilities\n",
    "# | means \"or\"\n",
    "len(re.findall(r'\\b(happy|smile)\\b',sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66bc719-02be-4f12-8aed-e71b78f9a7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q4 - prof demo - count how many time all the words are in the doc\n",
    "# 4.4.4 has examples + output\n",
    "# docstring: https://github.com/LeDataSciFi/ledatascifi-2023/blob/main/community_codebook/near_regex.py\n",
    "# solve \n",
    "\n",
    "len(re.findall(r'\\b(happy|smile|hopeful)\\b',sentence))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405428e0-938e-4227-91af-091fbfa690b9",
   "metadata": {},
   "source": [
    "That worked. Let's try to do the same using NEAR_regex.\n",
    "\n",
    "- look at docstring `help(NEAR_regex)` and 4.4.4 to learn about function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c8dc51-6790-4dc7-aea3-e4e66c1d1556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\b(happy|smile|hopeful)\\\\b)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for topic1 near topic2 \n",
    "# topic is a list of words, (word1|word2)a\n",
    "happy_sentiment_stringlist = ['(happy|smile|hopeful)']  # this is a list!!!!!\n",
    "NEAR_regex(happy_sentiment_stringlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cb009-8258-4da2-8301-5507581bf065",
   "metadata": {},
   "source": [
    "Warning!\n",
    "\n",
    "Check that the length of the \"stuff\" you give NEAR_regex <= 4. \n",
    "\n",
    "Updated version of function in codebook has a check at the top to prevent disaster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff03b70-9c2e-45e5-b68f-b4234c72a0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(NEAR_regex(happy_sentiment_stringlist)\n",
    "    ,sentence))\n",
    "\n",
    "# partial = true finds smiles\n",
    "# partal = false makes sense with well defined and large lists \n",
    "# max_words... how many is \"right\" \n",
    "# max_words = 5 ... means the words must be separated by at most 5, but the \"window\" around the anchor is 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f42fa877-20a1-4f7f-890a-fb55f84b9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q5 - using py's string functions, convert\n",
    "# happy_sentiment into the format NEAR_regex() wants \n",
    "# hint: 4.4.1\n",
    "\n",
    "happy_sentiment # take happy_sentiment, output this: '(happy|smile|hopeful)'\n",
    "happy_for_regex =      ['('+'|'.join(happy_sentiment)+')']     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c738e28-d4ad-49ea-8872-3bb3f6fbdabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q6 - calculate the doc's happy_sentiment score\n",
    "\n",
    "(\n",
    "len(re.findall(NEAR_regex(happy_for_regex),sentence)) \n",
    "/\n",
    "len(sentence.split())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087371c-09ab-44bc-96e7-3f1d3857556c",
   "metadata": {},
   "source": [
    "Now, you know how to \n",
    "1. Load the sentiment dictionaries\n",
    "1. Convert each to a form for NEAR_regex\n",
    "1. Use each regex to count hits in a doc\n",
    "1. Measure each sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9906de8-c99f-4651-9cfb-2a7ad8f4cc60",
   "metadata": {},
   "source": [
    "## Opening a 10-K file\n",
    "\n",
    "I'm giving everyone this code because dealing with Zips is a headache the first 15 times you do it.\n",
    "- Open the zip before the loop and get a list of all files already in it\n",
    "- With that zip open, do your loopy stuff inside it\n",
    "\n",
    "You'll update this loop with things we've learned already. Put the lego pieces together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3c63770-1ea8-4e97-96aa-03030e4751b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the zip file (do this before the for loop\n",
    "# so you only open it one time... faster)\n",
    "with ZipFile('10k_files/10k_files.zip','r') as zipfolder:\n",
    "    \n",
    "    # before the loop, get list of files in zipped folder\n",
    "    file_list = zipfolder.namelist()\n",
    "        \n",
    "    # replace this with how you'd loop over the dataframe\n",
    "    for firm in [\"ALLE\"]: #,\"MMM\",\"TSLA\"]:\n",
    "        \n",
    "        # get a list of possible files for this firm\n",
    "        firm_folder    = \"sec-edgar-filings/\" + firm + '/10-K/*/*.html'\n",
    "        possible_files = fnmatch.filter(file_list, firm_folder) \n",
    "        if len(possible_files) == 0: \n",
    "            continue\n",
    "            \n",
    "        fpath = possible_files[0] # the first match is the path to the file\n",
    "\n",
    "        # open the file (this is a little different!)\n",
    "        with zipfolder.open(fpath) as report_file:\n",
    "            html = report_file.read().decode(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9633d1-6074-4f45-a207-ec1e2633ea6f",
   "metadata": {},
   "source": [
    "## Cleaning the html\n",
    "\n",
    "Print out `html`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48b9ce50-afea-4414-8fc1-71499a31aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" ?><!--XBRL Document Created with Wdesk from Workiva--><!--Copyright 2022 Workiva--><!--r:55fcaf7e-a32c-46db-9214-d3455640547a,g:8d4d7be5-91d0-4fc9-ad03-d6b94bcac834,d:42dbe9d853384615ac6fc384cf0f23cd--><html xml:lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:alle=\"http://www.allegion.com/20211231\" xmlns:country=\"http://xbrl.sec.gov/country/2021\" xmlns:dei=\"http://xbrl.sec.gov/dei/2021q4\" xmlns:iso4217=\"http://www.xbrl.org/2003/iso4217\" xmlns:ix=\"http://www.xbrl.org/2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebf63d-588b-4b5c-8834-0d04019230f7",
   "metadata": {},
   "source": [
    "Regex won't work on this as is! We need to remove all the html tags, drop the hidden data, and then, with the remaining text, clean it up using the \"Good ideas\" in 4.4 and 4.4.4 of the book. However, we have to slightly adjust the code. \n",
    "\n",
    "1. Use BeautifulSoup() with the `lxml-xml` parser. Call the output `soup`. Don't use `get_text` yet. \n",
    "1. Delete the hidden XBRL \n",
    "\n",
    "    ```python\n",
    "    for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "        div.decompose()\n",
    "    ```\n",
    "    \n",
    "1. Continue on (get the text from the soup, and continue from there...)\n",
    "1. Check: My cleaned string says \"allegion\" in positions 396-404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "426ad006-18ee-4a08-b826-ac8a4c1e61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here\n",
    "\n",
    "soup = BeautifulSoup(html,features='lxml-xml')\n",
    "\n",
    "for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "    div.decompose()\n",
    "    \n",
    "lower = soup.get_text().lower()    \n",
    "no_punc = re.sub(r'\\W',' ',lower)    \n",
    "cleaned = re.sub(r'\\s+',' ',no_punc)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b70af-c46c-47dd-85f7-31c56c7f0cec",
   "metadata": {},
   "source": [
    "## Get 10-K dates\n",
    "\n",
    "We need to know when the 10-K is released to see the stock returns around it.\n",
    "\n",
    "I'm going to give you most of this code. How I figured it out:\n",
    "- I know we have the CIK and accession number\n",
    "- Looked for EDGAR urls that have CIK + accession number, and then list filing date on the page\n",
    "- https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n",
    "- `requests_html` ([my listed suggestion here](https://ledatascifi.github.io/ledatascifi-2023/content/04/01_Intro_to_scraping.html#my-suggestion)) is the `requests` module for getting data from the web PLUS the ability to grab parts of the html\n",
    "    \n",
    "Exercise:\n",
    "- I used code straight off the [documentation's home page](https://requests.readthedocs.io/projects/requests-html/en/latest/), adapted slightly. Look for examples that _find_ parts of the html.\n",
    "- You'll need to figure out the \"CSS Selector\"\n",
    "    - right click on the filing date on the webpage, click inspect\n",
    "    - in the area that popped up, right click on html code containing that date and copy the CSS selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc3b5286-1c2a-490c-aa1a-179d5cc22700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n"
     ]
    }
   ],
   "source": [
    "# before the loop, set up a browser session\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "\n",
    "# inside your loop, get the cik and accession number for the filing\n",
    "\n",
    "cik = 1122304\n",
    "accession_number = '0001193125-15-118890'\n",
    "\n",
    "# *one* way to get the filing date... \n",
    "\n",
    "url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}-index.html'\n",
    "print(url) # check it out...\n",
    "r = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9307f137-fa28-47d4-a45c-ff5b42d9d89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-04-03'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE: get the filing date out of this \"r\" object (one line of code will do)\n",
    "r.html.find('div.formGrouping:nth-child(1) > div:nth-child(2)',first=True).text\n",
    "\n",
    "# more readable, easier to understand, but... need to know a lil\n",
    "# soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# soup.find('div',text='Filing Date').find_next_sibling('div').text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34a7d5-f345-49df-93c3-13d46ad1a160",
   "metadata": {},
   "source": [
    "To use this in your actual midterm, save the  accession_number to your database while doing the main for loop to parse the text.\n",
    "\n",
    "Then, after that, I wrote a second for loop that loops over the rows, and uses the code above to grab the date. I added some error checking (What if we don't have an accession number for that firm, what if the url is wrong, or the server denies you, or the line of code with filing_date fails?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0af1b2-4d57-448b-8673-3e20d5cb2b50",
   "metadata": {},
   "source": [
    "## Get returns around the 10-K dates\n",
    "\n",
    "[Returns for 2022 are here.](https://github.com/LeDataSciFi/data/blob/main/Stock%20Returns%20(CRSP))\n",
    "\n",
    "Before you try to use that, here are toy datasets of returns and filing dates that mimic the structure of the data you'll actually have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2826dd22-cb38-4680-9bbe-317291406f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ticker': ['JJSF']*20 + ['TSLA']*20,\n",
    "    'date': ['2021-12-01', '2021-12-02', '2021-12-03', '2021-12-06', '2021-12-07', '2021-12-08', '2021-12-09', '2021-12-10', '2021-12-13', '2021-12-14', '2021-12-15', '2021-12-16', '2021-12-17', '2021-12-20', '2021-12-21', '2021-12-22', '2021-12-23', '2021-12-27', '2021-12-28', '2021-12-29'] + ['2022-12-02', '2022-12-05', '2022-12-06', '2022-12-07', '2022-12-08', '2022-12-09', '2022-12-12', '2022-12-13', '2022-12-14', '2022-12-15', '2022-12-16', '2022-12-19', '2022-12-20', '2022-12-21', '2022-12-22', '2022-12-23', '2022-12-27', '2022-12-28', '2022-12-29', '2022-12-30'],\n",
    "    'ret': [-0.011276, 0.030954, 0.000287, 0.014362, 0.012459, 0.017200, -0.010173, 0.011875, 0.012559, 0.002508, 0.022852, 0.012360, 0.017387, -0.008957, 0.016840, -0.000256, -0.002558, 0.009041, -0.002097, 0.010189] + [0.000822, -0.063687, -0.014415, -0.032143, -0.003447, 0.032345, -0.062720, -0.040937, -0.025784, 0.005548, -0.047187, -0.002396, -0.080536, -0.001669, -0.088828, -0.017551, -0.114089, 0.033089, 0.080827, 0.011164]\n",
    "}\n",
    "\n",
    "crsp = pd.DataFrame(data)\n",
    "crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "\n",
    "fake_filings = pd.DataFrame({'ticker':['JJSF','TSLA'],\n",
    "                             'filing_date':['2021-12-03','2022-12-13']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f495ed75-0584-4dbb-97f6-d5c74cf6e9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>-0.011276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>0.030954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>0.014362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>0.012459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>-0.010173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>0.011875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>0.012559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-14</td>\n",
       "      <td>0.002508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>0.022852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-16</td>\n",
       "      <td>0.012360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-17</td>\n",
       "      <td>0.017387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>-0.008957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>0.016840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>-0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>-0.002558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>0.009041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>-0.002097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-02</td>\n",
       "      <td>0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-05</td>\n",
       "      <td>-0.063687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>-0.014415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>-0.032143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>-0.003447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>0.032345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>-0.062720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-13</td>\n",
       "      <td>-0.040937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>-0.025784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>0.005548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>-0.047187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>-0.002396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>-0.080536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>-0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>-0.088828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>-0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>-0.114089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>0.033089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>0.080827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>0.011164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker       date       ret\n",
       "0    JJSF 2021-12-01 -0.011276\n",
       "1    JJSF 2021-12-02  0.030954\n",
       "2    JJSF 2021-12-03  0.000287\n",
       "3    JJSF 2021-12-06  0.014362\n",
       "4    JJSF 2021-12-07  0.012459\n",
       "5    JJSF 2021-12-08  0.017200\n",
       "6    JJSF 2021-12-09 -0.010173\n",
       "7    JJSF 2021-12-10  0.011875\n",
       "8    JJSF 2021-12-13  0.012559\n",
       "9    JJSF 2021-12-14  0.002508\n",
       "10   JJSF 2021-12-15  0.022852\n",
       "11   JJSF 2021-12-16  0.012360\n",
       "12   JJSF 2021-12-17  0.017387\n",
       "13   JJSF 2021-12-20 -0.008957\n",
       "14   JJSF 2021-12-21  0.016840\n",
       "15   JJSF 2021-12-22 -0.000256\n",
       "16   JJSF 2021-12-23 -0.002558\n",
       "17   JJSF 2021-12-27  0.009041\n",
       "18   JJSF 2021-12-28 -0.002097\n",
       "19   JJSF 2021-12-29  0.010189\n",
       "20   TSLA 2022-12-02  0.000822\n",
       "21   TSLA 2022-12-05 -0.063687\n",
       "22   TSLA 2022-12-06 -0.014415\n",
       "23   TSLA 2022-12-07 -0.032143\n",
       "24   TSLA 2022-12-08 -0.003447\n",
       "25   TSLA 2022-12-09  0.032345\n",
       "26   TSLA 2022-12-12 -0.062720\n",
       "27   TSLA 2022-12-13 -0.040937\n",
       "28   TSLA 2022-12-14 -0.025784\n",
       "29   TSLA 2022-12-15  0.005548\n",
       "30   TSLA 2022-12-16 -0.047187\n",
       "31   TSLA 2022-12-19 -0.002396\n",
       "32   TSLA 2022-12-20 -0.080536\n",
       "33   TSLA 2022-12-21 -0.001669\n",
       "34   TSLA 2022-12-22 -0.088828\n",
       "35   TSLA 2022-12-23 -0.017551\n",
       "36   TSLA 2022-12-27 -0.114089\n",
       "37   TSLA 2022-12-28  0.033089\n",
       "38   TSLA 2022-12-29  0.080827\n",
       "39   TSLA 2022-12-30  0.011164"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9c8b74f-0d1c-4e4a-bf3a-9e598f91cd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>filing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJSF</td>\n",
       "      <td>2021-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2022-12-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker filing_date\n",
       "0   JJSF  2021-12-03\n",
       "1   TSLA  2022-12-13"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try here...\n",
    "\n",
    "# pseudocode first! imagine the structure of the dataset you want and work backwards, you'll struggle otherwise!\n",
    "\n",
    "# this really is a paper and pencil problem\n",
    "fake_filings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce527-c804-4998-9d6b-ec69d32a0588",
   "metadata": {},
   "source": [
    "## Merge it all together\n",
    "\n",
    "The readme shows what the output dataset should look like, roughly. The midterm directions elaborate (10 sentiment variables, 2 return measures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6bb1-d32e-4ec9-a797-06c84556436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
